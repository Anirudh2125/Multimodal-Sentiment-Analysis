# Multimodal Sentiment Analysis System

## Objective
Develop a **multimodal sentiment analysis system** capable of jointly analyzing **textual and visual inputs** to classify sentiment and related semantic attributes such as humor, sarcasm, and offense.

---

## Task Breakdown
The project focuses on the following core tasks:
- Sentiment Classification  
- Humor, Sarcasm, and Offense Detection  
- Semantic Intensity Quantification  

Each task operates on combined text–image inputs and is evaluated using appropriate classification metrics.

---

## Task A: Sentiment Classification

### Objective
Classify the sentiment expressed in a multimodal input as **Positive** or **Negative**.

### Approach
- Encoded textual input using **Transformer-based language models (BERT)**.
- Extracted visual features using **ResNet-based CNNs** pretrained on image data.
- Designed a **multimodal fusion mechanism** to combine text and image embeddings for joint representation learning.
- Applied a sentiment classification layer on the fused representation.

### Input
- Text: Caption or embedded textual content  
- Image: Associated visual input  

### Output
- Positive  
- Negative  

### Evaluation
- Performance evaluated using **accuracy and F1-score**.

---

## Task B: Humor, Sarcasm, and Offense Detection

### Objective
Identify the presence of **humor**, **sarcasm**, and **offensive content** in multimodal inputs.

### Approach
- Leveraged fused text–image embeddings generated by the multimodal model.
- Performed multi-label classification to detect humor-related attributes.
- Evaluated predictions independently for each attribute.

### Output
- Humor: Present / Not Present  
- Sarcasm: Present / Not Present  
- Offense: Present / Not Present  

### Evaluation
- Evaluated using **macro F1-score** across humor-related categories.

---

## Task C: Semantic Intensity Quantification

### Objective
Quantify the **extent** to which humor-related attributes are expressed in a given multimodal input.

### Approach
- Extended the multimodal representation with a regression-based prediction layer.
- Estimated intensity scores corresponding to semantic attributes.

### Output
- Scaled values representing semantic intensity

### Evaluation
- Performance assessed using regression error metrics and consistency across samples.

---

## Results Summary
- Successfully implemented a **BERT–ResNet multimodal pipeline** for sentiment and semantic analysis.
- Achieved strong classification performance across sentiment and humor-related tasks.
- Demonstrated the effectiveness of multimodal fusion over unimodal baselines.
